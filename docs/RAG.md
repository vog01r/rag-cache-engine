# M√©thodologie RAG (Retrieval-Augmented Generation)

## üìñ Introduction

Le **RAG (Retrieval-Augmented Generation)** est une technique qui combine la recherche d'informations (retrieval) avec la g√©n√©ration de texte par des mod√®les de langage. Cette approche permet de cr√©er des syst√®mes de questions-r√©ponses plus pr√©cis, √† jour et v√©rifiables.

## üéØ Probl√®mes r√©solus par le RAG

### Limitations des LLM traditionnels
- **Connaissances fig√©es** : Entra√Ænement sur des donn√©es avec date de coupure
- **Hallucinations** : G√©n√©ration d'informations incorrectes ou invent√©es
- **Manque de sp√©cificit√©** : Difficult√©s avec des domaines tr√®s sp√©cialis√©s
- **Pas de sources** : Impossible de v√©rifier l'origine des informations

### Avantages du RAG
- ‚úÖ **Informations √† jour** : Acc√®s √† des documents r√©cents
- ‚úÖ **R√©duction des hallucinations** : Ancrage sur des sources r√©elles
- ‚úÖ **Tra√ßabilit√©** : Citation des sources utilis√©es
- ‚úÖ **Sp√©cialisation** : Adaptation √† des domaines sp√©cifiques
- ‚úÖ **Mise √† jour facile** : Ajout de nouveaux documents sans r√©entra√Ænement

## üîÑ Flux RAG classique

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. INGESTION    ‚îÇ
‚îÇ                 ‚îÇ
‚îÇ Documents  ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Chunking ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Embeddings ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Vector Store
‚îÇ (.pdf, .md)     ‚îÇ (500 tokens)   (sentence-bert)   (FAISS)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. RECHERCHE    ‚îÇ
‚îÇ                 ‚îÇ
‚îÇ Query ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Embedding ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Similarity Search ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Top-K Documents
‚îÇ (utilisateur)   (m√™me mod√®le)    (cosine/L2)        (k=3-5)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. G√âN√âRATION   ‚îÇ
‚îÇ                 ‚îÇ
‚îÇ Prompt = Context + Query ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ LLM ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Response
‚îÇ (documents + question)         (GPT/Claude)  (avec sources)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üèóÔ∏è Composants techniques

### 1. Embeddings (Vectorisation)

**Objectif** : Convertir le texte en repr√©sentations num√©riques capturant le sens s√©mantique.

**Mod√®les populaires** :
- `all-MiniLM-L6-v2` : L√©ger, rapide, 384 dimensions
- `all-mpnet-base-v2` : Plus pr√©cis, 768 dimensions
- `text-embedding-ada-002` (OpenAI) : Propri√©taire, tr√®s performant

**M√©triques de similarit√©** :
- **Cosinus** : Mesure l'angle entre vecteurs (ignore la magnitude)
- **Distance euclidienne (L2)** : Distance g√©om√©trique dans l'espace
- **Produit scalaire** : Sensible √† la magnitude

### 2. Vector Store (Base vectorielle)

**Objectif** : Stockage efficace et recherche rapide de vecteurs haute dimension.

**Solutions** :
- **FAISS** : Biblioth√®que Facebook, tr√®s rapide, en m√©moire ou sur disque
- **Chroma** : Base vectorielle open-source avec m√©tadonn√©es
- **Pinecone** : Service cloud sp√©cialis√©
- **Weaviate** : Base vectorielle avec GraphQL

**Algorithmes d'indexation** :
- **Flat (brute force)** : Pr√©cis mais lent pour grandes collections
- **IVF** : Index par quantification vectorielle
- **HNSW** : Graphes de mondes petits hi√©rarchiques

### 3. Chunking (Segmentation)

**Objectif** : Diviser les documents longs en segments coh√©rents pour l'embedding.

**Strat√©gies** :
- **Par taille fixe** : 500-1000 tokens avec overlap de 10-20%
- **Par structure** : Paragraphes, sections, phrases
- **S√©mantique** : Regroupement par similarit√© th√©matique
- **Hybride** : Combinaison de plusieurs approches

**Param√®tres cl√©s** :
- `chunk_size` : Taille des segments (balance pr√©cision/contexte)
- `overlap` : Chevauchement pour pr√©server la continuit√©
- `separators` : D√©limiteurs prioritaires (\n\n, \n, ., etc.)

### 4. Retrieval (R√©cup√©ration)

**Objectif** : Trouver les documents les plus pertinents pour une requ√™te.

**Approches** :
- **Dense retrieval** : Similarit√© vectorielle (embeddings)
- **Sparse retrieval** : Mots-cl√©s (BM25, TF-IDF)
- **Hybride** : Combinaison dense + sparse avec pond√©ration

**Filtrage** :
- **M√©tadonn√©es** : Date, auteur, type de document
- **Score de confiance** : Seuil de similarit√© minimum
- **Diversit√©** : Maximum Marginal Relevance (MMR)

### 5. G√©n√©ration

**Objectif** : Produire une r√©ponse naturelle bas√©e sur le contexte r√©cup√©r√©.

**Techniques de prompting** :
```
Syst√®me: Tu es un assistant utile qui r√©pond uniquement bas√© sur le contexte fourni.

Contexte:
[Document 1] Le RAG combine recherche et g√©n√©ration...
[Document 2] FAISS est une biblioth√®que vectorielle...

Question: Qu'est-ce que le RAG ?

Instructions:
- R√©ponds uniquement avec les informations du contexte
- Cite les documents utilis√©s
- Si l'info n'est pas disponible, dis-le clairement

R√©ponse:
```

## ‚ö° Optimisations et Cache

### Strat√©gies de cache

#### 1. Cache de r√©ponses
- **Cl√©** : Hash de la question + param√®tres (k, filtres)
- **Valeur** : R√©ponse compl√®te g√©n√©r√©e
- **TTL** : 1-24h selon la fra√Æcheur requise
- **√âviction** : LRU, LFU, ou TTL strict

#### 2. Cache d'embeddings
- **Cl√©** : Hash du texte
- **Valeur** : Vecteur d'embedding
- **Avantage** : √âvite le recalcul pour documents statiques
- **Stockage** : Redis, fichier local, base SQL

#### 3. Cache de recherche
- **Cl√©** : Hash de la requ√™te + param√®tres de recherche
- **Valeur** : Liste des documents r√©cup√©r√©s avec scores
- **Utilit√©** : Pour requ√™tes similaires ou variantes

### Optimisations de performance

#### Index vectoriel
```python
# FAISS - configuration pour la performance
import faiss

# Index plat (pr√©cis, lent pour >10K vecteurs)
index = faiss.IndexFlatL2(dimension)

# Index IVF (rapide, bon compromis)
nlist = 100  # nombre de clusters
quantizer = faiss.IndexFlatL2(dimension)
index = faiss.IndexIVFFlat(quantizer, dimension, nlist)

# Index HNSW (tr√®s rapide, plus de m√©moire)
index = faiss.IndexHNSWFlat(dimension, 32)
```

#### Chunking intelligent
```python
# Chunking avec overlap s√©mantique
def semantic_chunking(text, model, max_size=500, similarity_threshold=0.8):
    sentences = split_sentences(text)
    chunks = []
    current_chunk = []
    
    for sentence in sentences:
        if len(current_chunk) >= max_size:
            # V√©rifier la coh√©rence s√©mantique
            if semantic_similarity(current_chunk, [sentence]) < similarity_threshold:
                chunks.append(current_chunk)
                current_chunk = [sentence]
            else:
                current_chunk.append(sentence)
        else:
            current_chunk.append(sentence)
    
    return chunks
```

## üìä M√©triques et √©valuation

### M√©triques de r√©cup√©ration
- **Recall@k** : Proportion de documents pertinents dans les k premiers
- **Precision@k** : Proportion de documents pertinents parmi les k r√©cup√©r√©s
- **MRR (Mean Reciprocal Rank)** : Position moyenne du premier document pertinent
- **NDCG (Normalized DCG)** : Qualit√© du classement avec pertinence gradu√©e

### M√©triques de g√©n√©ration
- **BLEU** : Comparaison n-grammes avec r√©ponses de r√©f√©rence
- **ROUGE** : Overlap de s√©quences (r√©sum√©s)
- **BERTScore** : Similarit√© s√©mantique avec BERT
- **Faithfulness** : Fid√©lit√© au contexte fourni

### M√©triques syst√®me
- **Latence** : Temps de r√©ponse (ingestion, recherche, g√©n√©ration)
- **Throughput** : Requ√™tes par seconde
- **Cache hit rate** : Efficacit√© du syst√®me de cache
- **Co√ªt** : Appels API, compute, stockage

## üîÆ Am√©liorations avanc√©es

### 1. Recherche hybride
Combinaison de m√©thodes denses et sparse :
```python
def hybrid_search(query, alpha=0.7):
    # Recherche dense (embeddings)
    dense_results = vector_search(query, k=20)
    
    # Recherche sparse (BM25)
    sparse_results = bm25_search(query, k=20)
    
    # Fusion des scores
    final_results = []
    for doc in all_documents:
        dense_score = get_dense_score(doc, dense_results)
        sparse_score = get_sparse_score(doc, sparse_results)
        
        # Score hybride pond√©r√©
        hybrid_score = alpha * dense_score + (1-alpha) * sparse_score
        final_results.append((doc, hybrid_score))
    
    return sorted(final_results, key=lambda x: x[1], reverse=True)[:k]
```

### 2. Re-ranking
Am√©lioration du classement avec mod√®les sp√©cialis√©s :
```python
from sentence_transformers import CrossEncoder

# Mod√®le de re-ranking
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')

def rerank_documents(query, candidates):
    # Calcul des scores de pertinence
    pairs = [(query, doc['text']) for doc in candidates]
    scores = reranker.predict(pairs)
    
    # Nouveau classement
    for i, doc in enumerate(candidates):
        doc['rerank_score'] = scores[i]
    
    return sorted(candidates, key=lambda x: x['rerank_score'], reverse=True)
```

### 3. Query expansion
Enrichissement des requ√™tes pour am√©liorer la r√©cup√©ration :
```python
def expand_query(query, model):
    # G√©n√©ration de variantes
    expanded = model.generate(
        f"G√©n√©rez 3 reformulations de cette question: {query}",
        max_tokens=100
    )
    
    # Recherche avec toutes les variantes
    all_results = []
    for variant in [query] + expanded:
        results = vector_search(variant, k=5)
        all_results.extend(results)
    
    # D√©duplication et fusion
    return deduplicate_and_merge(all_results)
```

## üõ†Ô∏è Impl√©mentation en production

### Architecture scalable
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Load Balancer ‚îÇ    ‚îÇ   API Gateway   ‚îÇ    ‚îÇ   Auth Service  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                       ‚îÇ                       ‚îÇ
          ‚ñº                       ‚ñº                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   RAG Service   ‚îÇ‚óÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Vector Store   ‚îÇ‚óÄ‚îÄ‚îÄ‚ñ∂‚îÇ     Cache       ‚îÇ
‚îÇ   (FastAPI)     ‚îÇ    ‚îÇ   (Pinecone)    ‚îÇ    ‚îÇ    (Redis)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                       ‚îÇ                       ‚îÇ
          ‚ñº                       ‚ñº                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   LLM Service   ‚îÇ    ‚îÇ   Monitoring    ‚îÇ    ‚îÇ    Logging      ‚îÇ
‚îÇ  (OpenAI API)   ‚îÇ    ‚îÇ (Prometheus)    ‚îÇ    ‚îÇ (Elasticsearch) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Consid√©rations de s√©curit√©
- **Sanitisation** : Validation et nettoyage des inputs
- **Rate limiting** : Protection contre les abus
- **Authentification** : Contr√¥le d'acc√®s aux API
- **Chiffrement** : Donn√©es sensibles en transit et au repos
- **Audit** : Tra√ßabilit√© des requ√™tes et r√©ponses

### Monitoring et observabilit√©
```python
import time
from prometheus_client import Counter, Histogram, Gauge

# M√©triques personnalis√©es
query_counter = Counter('rag_queries_total', 'Total queries processed')
query_duration = Histogram('rag_query_duration_seconds', 'Query processing time')
cache_hit_rate = Gauge('rag_cache_hit_rate', 'Cache hit rate percentage')

def monitored_rag_query(query):
    start_time = time.time()
    query_counter.inc()
    
    try:
        result = rag_engine.answer(query)
        
        # Mise √† jour des m√©triques
        duration = time.time() - start_time
        query_duration.observe(duration)
        
        if result['cached']:
            cache_hit_rate.inc()
            
        return result
        
    except Exception as e:
        logger.error(f"RAG query failed: {e}")
        raise
```

Ce guide fournit une base solide pour comprendre et impl√©menter des syst√®mes RAG robustes et performants. La d√©monstration de ce projet illustre ces concepts dans un contexte pratique et extensible.
