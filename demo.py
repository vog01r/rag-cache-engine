#!/usr/bin/env python3
"""
Script de d√©monstration pour le syst√®me RAG avec cache.
Montre un exemple complet: ingestion -> vectorisation -> requ√™te -> r√©ponse.
"""

import os
import sys
from pathlib import Path

# Ajout du r√©pertoire src au path Python
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.embedder import SentenceTransformersEmbedder
from src.vector_store import VectorStore
from src.llm_adapter import MockLLM
from src.rag_engine import RAGEngine
from src.utils import (
    setup_logging, load_config, load_documents_from_directory, 
    chunk_documents, ensure_directory
)


def print_separator(title: str) -> None:
    """Affiche un s√©parateur avec titre."""
    print(f"\n{'='*60}")
    print(f" {title}")
    print(f"{'='*60}")


def print_subsection(title: str) -> None:
    """Affiche un sous-titre."""
    print(f"\n--- {title} ---")


def main():
    """Fonction principale de d√©monstration."""
    print_separator("D√âMONSTRATION RAG AVEC CACHE")
    print("Ce script d√©montre un syst√®me RAG complet avec:")
    print("- Ingestion de documents")
    print("- Vectorisation avec sentence-transformers")
    print("- Recherche vectorielle avec FAISS")
    print("- Cache intelligent des r√©ponses")
    print("- G√©n√©ration mock√©e de r√©ponses LLM")
    
    # 1. Configuration et logging
    print_subsection("1. Configuration")
    setup_logging("INFO")
    config = load_config()
    print(f"‚úì Configuration charg√©e")
    
    # 2. Initialisation des composants
    print_subsection("2. Initialisation des composants")
    
    # Embedder
    embedder = SentenceTransformersEmbedder(
        model_name=config['embedding']['model_name']
    )
    print(f"‚úì Embedder initialis√©: {config['embedding']['model_name']}")
    
    # Vector store
    vector_store = VectorStore(dimension=config['vector_store']['dimension'])
    print(f"‚úì Vector store initialis√© (dimension: {config['vector_store']['dimension']})")
    
    # LLM adapter (mock)
    llm_adapter = MockLLM()
    print(f"‚úì LLM adapter mock√© initialis√©")
    
    # RAG Engine
    rag_engine = RAGEngine(
        embedder=embedder,
        vector_store=vector_store,
        llm_adapter=llm_adapter,
        k=config['retrieval']['k'],
        enable_cache=True,
        cache_ttl=config['cache']['ttl_seconds']
    )
    print(f"‚úì Moteur RAG initialis√© (k={config['retrieval']['k']}, cache activ√©)")
    
    # 3. Chargement et ingestion des documents
    print_subsection("3. Chargement des documents")
    
    docs_dir = "samples/docs_to_ingest"
    if not os.path.exists(docs_dir) or len(os.listdir(docs_dir)) == 0:
        print(f"‚ö†Ô∏è  R√©pertoire {docs_dir} vide ou non trouv√©. Cr√©ation d'exemples...")
        create_sample_documents()
    
    # Chargement des documents
    documents = load_documents_from_directory(docs_dir)
    print(f"‚úì Charg√© {len(documents)} documents depuis {docs_dir}")
    
    for doc in documents:
        filename = doc.get('filename', 'inconnu')
        text_length = len(doc.get('text', ''))
        print(f"  - {filename}: {text_length} caract√®res")
    
    # Chunking des documents
    print_subsection("4. Chunking et ingestion")
    chunked_docs = chunk_documents(
        documents, 
        max_tokens=config['chunking']['max_tokens'],
        overlap=config['chunking']['overlap']
    )
    print(f"‚úì Documents divis√©s en {len(chunked_docs)} chunks")
    
    # Ingestion dans le RAG
    rag_engine.ingest_documents(chunked_docs)
    print(f"‚úì {len(chunked_docs)} chunks ing√©r√©s dans le syst√®me RAG")
    
    # 5. D√©monstration des requ√™tes
    print_separator("D√âMONSTRATION DES REQU√äTES")
    
    # Questions d'exemple
    sample_queries = [
        "Qu'est-ce que le RAG ?",
        "Comment fonctionne la vectorisation ?",
        "Quels sont les avantages du cache ?",
        "Qu'est-ce que FAISS ?",
        "Comment am√©liorer les performances ?"
    ]
    
    for i, query in enumerate(sample_queries, 1):
        print_subsection(f"Requ√™te {i}: {query}")
        
        # Premi√®re ex√©cution (sans cache)
        result = rag_engine.answer(query)
        
        print(f"Temps de traitement: {result['processing_time']:.3f}s")
        print(f"Documents trouv√©s: {len(result['documents'])}")
        print(f"Mise en cache: {'Oui' if not result['cached'] else '√âtait d√©j√† en cache'}")
        print(f"Longueur du contexte: {result.get('context_length', 0)} caract√®res")
        
        # Affichage des sources
        if result['documents']:
            print("\nSources utilis√©es:")
            for j, doc in enumerate(result['documents'][:3], 1):  # Top 3
                source = doc.get('source', 'Source inconnue')
                distance = doc.get('distance', 0)
                print(f"  {j}. {os.path.basename(source)} (distance: {distance:.3f})")
        
        # Affichage de la r√©ponse (tronqu√©e)
        answer = result['answer']
        if len(answer) > 300:
            answer = answer[:300] + "..."
        print(f"\nR√©ponse:\n{answer}")
        
        # Test du cache (deuxi√®me ex√©cution de la m√™me requ√™te)
        if i == 1:  # Test cache seulement pour la premi√®re requ√™te
            print("\nüîÑ Test du cache (m√™me requ√™te)...")
            cached_result = rag_engine.answer(query)
            print(f"Temps avec cache: {cached_result['processing_time']:.3f}s")
            print(f"R√©cup√©r√© du cache: {'Oui' if cached_result['cached'] else 'Non'}")
    
    # 6. Statistiques finales
    print_separator("STATISTIQUES DU SYST√àME")
    stats = rag_engine.get_statistics()
    
    print(f"Documents dans le vector store: {stats['vector_store_size']}")
    print(f"Mod√®le d'embedding: {stats['embedder_model']}")
    print(f"Nombre de documents r√©cup√©r√©s (k): {stats['retrieval_k']}")
    print(f"Cache activ√©: {'Oui' if stats['cache_enabled'] else 'Non'}")
    
    if 'cache_stats' in stats:
        cache_stats = stats['cache_stats']
        print(f"Entr√©es en cache: {cache_stats['size']}/{cache_stats['max_size']}")
        print(f"TTL du cache: {cache_stats['ttl_seconds']}s")
    
    # 7. Test de sauvegarde (optionnel)
    print_subsection("7. Test de sauvegarde")
    save_dir = "demo_vector_store"
    ensure_directory(save_dir)
    vector_store.save(save_dir)
    print(f"‚úì Vector store sauvegard√© dans {save_dir}")
    
    print_separator("D√âMONSTRATION TERMIN√âE")
    print("üéâ La d√©monstration s'est termin√©e avec succ√®s !")
    print("\nProchaines √©tapes possibles:")
    print("- Examiner le code dans src/")
    print("- Lancer les tests: pytest tests/")
    print("- Lire la documentation: docs/RAG.md")
    print("- Modifier config.yaml pour ajuster les param√®tres")
    print("- Ajouter vos propres documents dans samples/docs_to_ingest/")


def create_sample_documents():
    """Cr√©e des documents d'exemple pour la d√©monstration."""
    ensure_directory("samples/docs_to_ingest")
    
    # Document 1: Introduction au RAG
    doc1_content = """# Introduction au RAG (Retrieval-Augmented Generation)

Le RAG est une technique qui combine la recherche d'informations avec la g√©n√©ration de texte. 
Cette approche permet aux mod√®les de langage d'acc√©der √† des bases de connaissances externes 
pour produire des r√©ponses plus pr√©cises et √† jour.

## Principe de fonctionnement

1. **Ingestion**: Les documents sont trait√©s et convertis en vecteurs
2. **Indexation**: Les vecteurs sont stock√©s dans une base vectorielle
3. **Recherche**: Pour une question, on trouve les documents les plus pertinents
4. **G√©n√©ration**: Le LLM utilise ces documents comme contexte pour r√©pondre

## Avantages du RAG

- Acc√®s √† des informations sp√©cifiques et r√©centes
- R√©duction des hallucinations du mod√®le
- Possibilit√© de citer les sources
- Mise √† jour facile de la base de connaissances
"""

    # Document 2: Vectorisation et embeddings
    doc2_content = """# Vectorisation et Embeddings

La vectorisation est le processus de conversion de texte en repr√©sentations num√©riques 
que les machines peuvent traiter efficacement.

## Qu'est-ce qu'un embedding ?

Un embedding est un vecteur de nombres r√©els qui repr√©sente le sens s√©mantique d'un texte.
Les textes similaires ont des embeddings proches dans l'espace vectoriel.

## Mod√®les d'embedding

- **Sentence-BERT**: Optimis√© pour les phrases et paragraphes
- **OpenAI Ada**: Mod√®le propri√©taire performant
- **all-MiniLM-L6-v2**: Mod√®le compact et efficace (utilis√© dans cette d√©mo)

## M√©triques de similarit√©

- Distance euclidienne
- Similarit√© cosinus
- Distance de Manhattan

FAISS utilise g√©n√©ralement la distance L2 (euclidienne) pour la recherche rapide.
"""

    # Document 3: Cache et optimisation
    doc3_content = """# Strat√©gies de Cache et Optimisation

Le cache est crucial pour optimiser les performances d'un syst√®me RAG en production.

## Types de cache

### Cache de r√©ponses
- Stocke les r√©ponses compl√®tes pour des questions identiques
- TTL (Time To Live) configurable
- √âvite les appels r√©p√©t√©s au LLM

### Cache de vecteurs
- Stocke les embeddings calcul√©s
- √âvite le recalcul pour des textes identiques
- Particuli√®rement utile pour les documents statiques

## Algorithmes d'√©viction

- **LRU (Least Recently Used)**: Supprime les entr√©es les moins r√©cemment utilis√©es
- **LFU (Least Frequently Used)**: Supprime les entr√©es les moins fr√©quemment utilis√©es
- **TTL**: Supprime les entr√©es expir√©es

## Optimisations suppl√©mentaires

- Indexation hi√©rarchique
- Filtrage par m√©tadonn√©es
- Recherche hybride (dense + sparse)
- Quantification des vecteurs
"""

    # Sauvegarde des documents
    documents = [
        ("introduction_rag.md", doc1_content),
        ("vectorisation.md", doc2_content),
        ("cache_optimisation.md", doc3_content)
    ]
    
    for filename, content in documents:
        filepath = os.path.join("samples/docs_to_ingest", filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"‚úì Document d'exemple cr√©√©: {filename}")


if __name__ == "__main__":
    main()
